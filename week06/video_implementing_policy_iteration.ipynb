{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCaN3DcBUYWa"
      },
      "source": [
        "# Video: Implementing Policy Iteration\n",
        "\n",
        "Policy iteration methods are a common method to solve modest-sized Markov decision problems.\n",
        "They build on the ideas of value iteration methods, but directly calculate a policy so the policy does not need to be reconstructed later.\n",
        "This video will walk through implementing policy iteration and a faster variation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS3u_A46kCCa"
      },
      "source": [
        "Script: (faculty on screen)\n",
        "* Policy iteration methods are a common method to solve modest-sized Markov decision problems.\n",
        "* In this video, I will show you how to implement policy iteration and common speedups to the base algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ngDaNSLkJJA"
      },
      "source": [
        "## Problem Setup\n",
        "\n",
        "Problem size:\n",
        "* $k$ actions\n",
        "* $n$ states\n",
        "\n",
        "Problem dynamics:\n",
        "* $R: k \\times n$ array of expected rewards for each action+state\n",
        "* $P: k \\times n \\times n$ array of transition probabilities for each action/current state/next state.\n",
        "* $\\gamma$ discount factor for future rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwwQcSCBk8E1"
      },
      "source": [
        "Script:\n",
        "* Policy iteration considers the same problems as value iteration.\n",
        "* What is the optimal behavior for a Markov decision problem?\n",
        "* Policy iteration focuses on computing an optimal policy where value iteration focused on optimal values.\n",
        "* You will see that policy iteration also computes optimal values along the way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV-6JZCqlu-I"
      },
      "source": [
        "## Policy Iteration Algorithm\n",
        "\n",
        "1. Pick arbitrary initial policy $\\pi_0$.\n",
        "2. Set $\\mathbf{v}_0 = \\mathbf{v}_{\\pi_0}$ using iterative policy evaluation.\n",
        "3. For $i = 0, 1, 2, \\ldots$ until policy stops changing:\n",
        "\\begin{array}{rcl}\n",
        "\\pi_{i+1}(s) & = & \\mathrm{argmax}_{a \\in A(s)} \\mathcal{R}^a + \\gamma \\mathcal{P}^a v_i \\\\\n",
        "\\mathbf{v}_{i+1}(s) & = & \\mathbf{v}_{\\pi_{i+1}}~\\text{using value iteration}\n",
        "\\end{array}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIv2QiSEok1S"
      },
      "source": [
        "Script:\n",
        "* Here is the basic form of policy iteration.\n",
        "* First, you start with an arbitrary first policy, $\\pi_0$.\n",
        "* This first policy can be anything.\n",
        "* It could pick actions uniformly at random, or just pick the first action.\n",
        "* The main loop of policy iteration will deterministic policies, so it will be more convenient to start with a deterministic policy too.\n",
        "* After picking the first policy, use iterative policy evaluation to compute the values under that policy.\n",
        "* Then the main loop alternates between improving the policy given those values, and improving the values given the new policy.\n",
        "* Like with value iteration, policy iteration runs until the changes stop or become small.\n",
        "* Let's look at some code now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QpsdCTfwsq0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0pusdk9wtPL"
      },
      "outputs": [],
      "source": [
        "# copied from Implementing Value Iteration video\n",
        "def compute_qT_once(R, P, gamma, v):\n",
        "    return R + gamma * P @ v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8jqMWPMwtBl"
      },
      "outputs": [],
      "source": [
        "# copied from Implementing Value Iteration video\n",
        "def iterate_values_once(R, P, gamma, v):\n",
        "    return np.max(compute_qT_once(R, P, gamma, v), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSaNo9cD092j"
      },
      "outputs": [],
      "source": [
        "# copied from Implementing Value Iteration video\n",
        "def value_iteration(R, P, gamma, max_iterations=100, tolerance=0.001):\n",
        "    # initial approximation v_0\n",
        "    v_old = np.zeros(R.shape[-1])\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        # compute v_{i+1}\n",
        "        v_new = iterate_values_once(R, P, gamma, v_old)\n",
        "\n",
        "        # check if values did not change much\n",
        "        if np.max(np.abs(v_new - v_old)) < tolerance:\n",
        "            return v_new\n",
        "\n",
        "        v_old = v_new\n",
        "\n",
        "    # return v_{max_iterations}\n",
        "    return v_old"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NppgDPTzw6eX"
      },
      "outputs": [],
      "source": [
        "# copied from Implementing Iterative Policy Evaluation Video\n",
        "def iterative_policy_evaluation(R, P, gamma, pi, max_iterations=100, tolerance=0.001):\n",
        "    # factor out action choices using policy.\n",
        "\n",
        "    # deterministic version\n",
        "    n = R.shape[-1]\n",
        "    R_pi = R[pi, np.arange(n)]\n",
        "    P_pi = P[pi, np.arange(n),:]\n",
        "\n",
        "    # reshape to one dummy action to reuse previous example code\n",
        "    R_pi = R_pi.reshape(1, *R_pi.shape)\n",
        "    P_pi = P_pi.reshape(1, *P_pi.shape)\n",
        "\n",
        "    # initial approximation v_0\n",
        "    v_old = np.zeros(R.shape[-1])\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        # compute v_{i+1}\n",
        "        v_new = iterate_values_once(R_pi, P_pi, gamma, v_old)\n",
        "\n",
        "        # check if values did not change much\n",
        "        if np.max(np.abs(v_new - v_old)) < tolerance:\n",
        "            return v_new\n",
        "\n",
        "        v_old = v_new\n",
        "\n",
        "    # return v_{max_iterations}\n",
        "    return v_old"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwFHk1WTyUWL"
      },
      "source": [
        "Script:\n",
        "* I copied some of the code from the previous videos on value iteration and iterative policy evaluation.\n",
        "* We will be using it with some modifications for policy iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bUNWg5-xAXI"
      },
      "outputs": [],
      "source": [
        "def policy_iteration_baseline(R, P, gamma, max_iterations=100, tolerance=0.001):\n",
        "    pi_old = np.zeros(R.shape[-1], dtype=\"int64\")\n",
        "    v_old = iterative_policy_evaluation(R, P, gamma, pi_old)\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        # compute new policy\n",
        "        pi_new = np.argmax(compute_qT_once(R, P, gamma, v_old), axis=0)\n",
        "        if np.array_equal(pi_new, pi_old):\n",
        "            return pi_new, v_old\n",
        "\n",
        "        # compute new values\n",
        "        v_new = iterative_policy_evaluation(R, P, gamma, pi_new)\n",
        "        if np.max(np.abs(v_new - v_old)) < tolerance:\n",
        "            return pi_new, v_new\n",
        "\n",
        "        pi_old = pi_new\n",
        "        v_old = v_new\n",
        "\n",
        "    return pi_old, v_old"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNYLJQ31w_aB"
      },
      "source": [
        "Script:\n",
        "* I copied and modified the value iteration code for the first version of policy iteration.\n",
        "* The big changes are that the values are always computed from the latest policy.\n",
        "* The policy updates are pretty similar to the old value updates; they just change the max to an argmax.\n",
        "* Let's test it out on our example environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urA8dPL7yd-j"
      },
      "source": [
        "## Example Environment\n",
        "\n",
        "<table>\n",
        "<tr><td align=\"right\">0\ud83e\uddca</td><td align=\"right\">1\ud83e\uddca</td><td align=\"right\">2\ud83e\uddca</td><td align=\"right\">3\ud83e\uddca</td><td align=\"right\">4\ud83e\uddca</td><td align=\"right\">5\ud83e\uddca</td><td align=\"right\">6\ud83e\uddca</td></tr>\n",
        "<tr><td align=\"right\">7\ud83e\uddca</td><td align=\"right\">8\ud83e\uddca</td><td align=\"right\">9\ud83e\uddca</td><td align=\"right\">10\ud83e\uddca</td><td align=\"right\">11\ud83e\uddca</td><td align=\"right\">12\ud83e\uddca</td><td align=\"right\">13\ud83e\uddca</td></tr>\n",
        "<tr><td align=\"right\">14\ud83e\uddca</td><td align=\"right\">15\ud83e\uddca</td><td align=\"right\">16\ud83e\uddca</td><td align=\"right\">17\ud83e\uddca</td><td align=\"right\">18\ud83e\uddca</td><td align=\"right\">19\ud83e\uddca</td><td align=\"right\">20\ud83e\uddca</td></tr>\n",
        "<tr><td align=\"right\">21\ud83e\uddca</td><td align=\"right\">22\ud83e\uddca</td><td align=\"right\">23\ud83e\uddca</td><td align=\"right\">24\ud83e\uddca</td><td align=\"right\">25\ud83e\uddca</td><td align=\"right\">26\ud83e\uddca</td><td align=\"right\">27\ud83e\uddca</td></tr>\n",
        "<tr><td align=\"right\">28\ud83e\uddca</td><td align=\"right\">29\ud83e\uddca</td><td align=\"right\">30\ud83e\uddca</td><td align=\"right\">31\ud83e\uddca</td><td align=\"right\">32\ud83e\uddca</td><td align=\"right\">33\ud83e\uddca</td><td align=\"right\">34\ud83e\uddca</td></tr>\n",
        "<tr><td align=\"right\">35\ud83e\uddca</td><td align=\"right\">36\ud83e\uddca</td><td align=\"right\">37\ud83e\uddca</td><td align=\"right\">38\ud83e\uddca</td><td align=\"right\">39\ud83e\uddca</td><td align=\"right\">40\ud83e\uddca</td><td align=\"right\">41\ud83e\uddca</td></tr>\n",
        "<tr><td align=\"right\">42\ud83e\uddca</td><td align=\"right\">43\ud83e\uddca</td><td align=\"right\">44\ud83e\uddca</td><td>45\ud83e\uddca</td><td align=\"right\">46\ud83e\uddca</td><td align=\"right\">47\ud83e\uddca</td><td align=\"right\">48\ud83d\udc1f</td></tr>\n",
        "<tr><td></td></tr>\n",
        "<tr><td align=\"right\">49\u2611\ufe0f</td></tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj8ZGc9XygHh"
      },
      "source": [
        "Script:\n",
        "* Here is our beloved hungry penguin environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARHaecN1ypgs"
      },
      "outputs": [],
      "source": [
        "actions = [\"\u2b06\ufe0f\", \"\u2b07\ufe0f\", \"\u2b05\ufe0f\", \"\u27a1\ufe0f\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe-ATt_Pyx6D"
      },
      "outputs": [],
      "source": [
        "P = np.zeros(shape=(len(actions), 50, 50))\n",
        "for s in range(48):\n",
        "    # row major order with (0,0) as top left\n",
        "    x = s % 7\n",
        "    y = s // 7\n",
        "\n",
        "    # up action\n",
        "    if y > 0:\n",
        "        P[0, s, s-7] = 1\n",
        "    else:\n",
        "        P[0, s, s] = 1\n",
        "\n",
        "    # down action\n",
        "    if y < 6:\n",
        "        P[1, s, s+7] = 1\n",
        "    else:\n",
        "        P[1, s, s] = 1\n",
        "\n",
        "    # left action\n",
        "    if x > 0:\n",
        "        P[2, s, s-1] = 1\n",
        "    else:\n",
        "        P[2, s, s] = 1\n",
        "\n",
        "    # right action\n",
        "    if x < 6:\n",
        "        P[3, s, s+1] = 1\n",
        "    else:\n",
        "        P[3, s, s] = 1\n",
        "\n",
        "# fish state goes to terminal state\n",
        "P[:,48,49] = 1\n",
        "\n",
        "# stay in terminal state\n",
        "P[:,49,49] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImlDA_fty3H3"
      },
      "outputs": [],
      "source": [
        "R = np.zeros(shape=(4,50))\n",
        "R[:,48] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPHQETtYy3k0"
      },
      "outputs": [],
      "source": [
        "gamma = 0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2UBDmR8y957"
      },
      "source": [
        "Script:\n",
        "* I setup the rewards and transitions the same as when we were looking at value iteration.\n",
        "* Let's test out the policy iteration now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vMFx-_Vy_MU",
        "outputId": "0f5d27c2-f4b0-4afa-e109-8c5199dfec2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3,\n",
              "       3, 3, 3, 3, 0, 0])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(pi_star, v_star) = policy_iteration_baseline(R, P, gamma)\n",
        "pi_star"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnpTXy0KzKBH"
      },
      "source": [
        "Script:\n",
        "* Policy iteration can return both a policy and values to match since it computes both of them.\n",
        "* This policy is different from our previous example.\n",
        "* Let's visualize it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqytbstTzQPD",
        "outputId": "544e1dc6-15c3-43e8-c7ff-854b8d4c1a6b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([['\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f'],\n",
              "       ['\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f'],\n",
              "       ['\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f'],\n",
              "       ['\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f'],\n",
              "       ['\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f'],\n",
              "       ['\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f', '\u2b07\ufe0f'],\n",
              "       ['\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u2b06\ufe0f']], dtype='<U2')"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.asarray([actions[a] for a in pi_star[:49]]).reshape(7, 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io7n02oxzWxd"
      },
      "source": [
        "Script:\n",
        "* This policy is optimal.\n",
        "* It goes down first instead of right first, but that does not change the optimality for this problem.\n",
        "* There is an up action at the fish location, but don't worry.\n",
        "* The action does not matter for that state.\n",
        "* Let's check the values too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "NLd1pYHVzqNr",
        "outputId": "49fa44ec-dc6f-46bc-d45c-9dd230172511"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(v_star[:49].reshape(7, 7));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8l7D1jkztx1"
      },
      "source": [
        "Script:\n",
        "* That looks like the same optimal values that we have previously looked at."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7U7bTx4onZs"
      },
      "source": [
        "## Policy Iteration Algorithm with Warm Start\n",
        "\n",
        "1. Pick arbitrary initial policy $\\pi_0$ (e.g. uniform random actions).\n",
        "2. Set $\\mathbf{v}_0 = \\mathbf{v}_{\\pi_0}$ using iterative policy evaluation starting from zero values.\n",
        "3. For $i = 0, 1, 2, \\ldots$ until policy stops changing:\n",
        "\\begin{array}{rcl}\n",
        "\\pi_{i+1}(s) & = & \\mathrm{argmax}_{a \\in A(s)} \\mathcal{R}^a + \\gamma \\mathcal{P}^a \\mathbf{v}_i(s) \\\\\n",
        "\\mathbf{v}_{i+1} & = & \\mathbf{v}_{\\pi_{i+1}}~\\text{using iterative policy evaluation starting from}~\\mathbf{v}_i \\\\\n",
        "\\end{array}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pt8VgiXpICp"
      },
      "source": [
        "Script:\n",
        "* The most expensive step of policy iteration is the value updates.\n",
        "* We can speed them up by starting them from the previous set of values.\n",
        "* To do so, we will have to update our iterative policy evaluation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49NZ8WeR0IZS"
      },
      "outputs": [],
      "source": [
        "# copied from Implementing Iterative Policy Evaluation Video\n",
        "def iterative_policy_evaluation(R, P, gamma, pi, max_iterations=100, tolerance=0.001, warmstart=None):\n",
        "    # factor out action choices using policy.\n",
        "\n",
        "    # deterministic version\n",
        "    n = R.shape[-1]\n",
        "    R_pi = R[pi, np.arange(n)]\n",
        "    P_pi = P[pi, np.arange(n),:]\n",
        "\n",
        "    # reshape to one dummy action to reuse previous example code\n",
        "    R_pi = R_pi.reshape(1, *R_pi.shape)\n",
        "    P_pi = P_pi.reshape(1, *P_pi.shape)\n",
        "\n",
        "    # initial approximation v_0\n",
        "    v_old = warmstart if warmstart is not None else np.zeros(R.shape[-1])\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        # compute v_{i+1}\n",
        "        v_new = iterate_values_once(R_pi, P_pi, gamma, v_old)\n",
        "\n",
        "        # check if values did not change much\n",
        "        if np.max(np.abs(v_new - v_old)) < tolerance:\n",
        "            return v_new\n",
        "\n",
        "        v_old = v_new\n",
        "\n",
        "    # return v_{max_iterations}\n",
        "    return v_old"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eR4CMcz2ImO"
      },
      "source": [
        "Script:\n",
        "* I just added a warmstart parameter, and changed the value initialization to use it if it is present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAIXeCmCz_QN"
      },
      "outputs": [],
      "source": [
        "def policy_iteration_warmstart(R, P, gamma, max_iterations=100, tolerance=0.001):\n",
        "    pi_old = np.zeros(R.shape[-1], dtype=\"int64\")\n",
        "    v_old = iterative_policy_evaluation(R, P, gamma, pi_old)\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        # compute new policy\n",
        "        pi_new = np.argmax(compute_qT_once(R, P, gamma, v_old), axis=0)\n",
        "        if np.array_equal(pi_new, pi_old):\n",
        "            return pi_new, v_old\n",
        "\n",
        "        # compute new values\n",
        "        v_new = iterative_policy_evaluation(R, P, gamma, pi_new, warmstart=v_old)\n",
        "        if np.max(np.abs(v_new - v_old)) < tolerance:\n",
        "            return pi_new, v_new\n",
        "\n",
        "        pi_old = pi_new\n",
        "        v_old = v_new\n",
        "\n",
        "    return pi_old, v_old"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi9QQTyT0Vf8"
      },
      "source": [
        "Script:\n",
        "* The new policy iteration version just adds that warmstart parameter when calling it in the main loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_j1fS1B0Uw8",
        "outputId": "b4f1e34b-8ff1-47db-b88a-63227ee4f6c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3,\n",
              "       3, 3, 3, 3, 0, 0])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(pi_warmstart, v_warmstart) = policy_iteration_warmstart(R, P, gamma)\n",
        "pi_warmstart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCsCBe180gWO"
      },
      "source": [
        "Script:\n",
        "* We get the same policy out now, but it's slightly faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suXUmNKj0hqJ"
      },
      "source": [
        "Script: (faculty on camera)\n",
        "* Policy iteration is a useful method for computing optimal policies and values.\n",
        "* There are many variations of policy iteration.\n",
        "* The warmstart version that we just covered will always be an improvement over the baseline.\n",
        "* Other variations may be useful more situationally."
      ]
    }
  ],
  "metadata": {
    "colab": {},
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}