{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1m1xdQjUW-3"
      },
      "source": [
        "# Video: Implementing Iterative Policy Evaluation\n",
        "\n",
        "Iterative policy evaluation is an algorithm for computing state values while following a given policy.\n",
        "This video will show you how to implement iterative policy evaluation while making connections to the value iteration method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIGqK3FuUk8j"
      },
      "source": [
        "Script: (faculty on screen)\n",
        "* Iterative policy evaluation is an algorithm for computing state values while following a given policy.\n",
        "* It has some similarities to value iteration, but instead of computing optimal values under any policy, it computes the values of a specified policy.\n",
        "* The resulting computation is similar to value iteration, but can be much faster if there are many actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3fP_5imUdp6"
      },
      "source": [
        "## Problem Setup\n",
        "\n",
        "Problem size:\n",
        "* $k$ actions\n",
        "* $n$ states\n",
        "\n",
        "Problem dynamics:\n",
        "* $R: k \\times n$ array of expected rewards for each action+state\n",
        "* $P: k \\times n \\times n$ array of transition probabilities for each action/current state/next state.\n",
        "* $\\gamma$ discount factor for future rewards\n",
        "\n",
        "New:\n",
        "* $\\pi: n$ array of state actions (a deterministic policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcn3wuFfVW9Q"
      },
      "source": [
        "Script:\n",
        "* The problem setup for iterative policy evaluation is similar to optimal value problems.\n",
        "* We are given an array of expected rewards for each action and state, and an array of transition probabilities for each action.\n",
        "* We are also given a discount factor gamma.\n",
        "* The change from optimal value problems is that we are also given a policy, and we will be computing the value subject to following that policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ApvP8Z7VYeI"
      },
      "source": [
        "## Factoring out Action Choices using Policy\n",
        "\n",
        "\\begin{array}{rcl}\n",
        "\\mathcal{R}^\\pi_i & = & \\sum_a \\pi(i, a) \\mathcal{R}^a_i \\\\\n",
        "\\mathcal{P}^\\pi_{i,j} & = & \\sum_a \\pi(i,a) \\mathcal{P}^a_{i, j}\n",
        "\\end{array}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4pTDf1fWYk9"
      },
      "source": [
        "Script:\n",
        "* The first thing that we do with value iteration is compute the expected rewards and transition probabilities conditioned on following the given policy.\n",
        "* This reduces our Markov decision process to a Markov reward policy or a Markov decision process with just one action.\n",
        "* We will use the latter structure and reuse some of our value iteration code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd9smo4hW-HV"
      },
      "source": [
        "## Iterative Policy Evaluation Algorithm\n",
        "\n",
        "1. Factor out the policy action choices.\n",
        "\\begin{array}{rcl}\n",
        "\\mathcal{R}^\\pi_i & = & \\sum_a \\pi(i, a) \\mathcal{R}^a_i \\\\\n",
        "\\mathcal{P}^\\pi_{i,j} & = & \\sum_a \\pi(i,a) \\mathcal{P}^a_{i, j}\n",
        "\\end{array}\n",
        "2. Initialize $v_0 = [0,\\ldots, 0]$.\n",
        "3. Repeat until values stop changing:\n",
        "\\begin{array}{rcl}\n",
        "v_{i+1}&=& \\mathcal{R}^\\pi + \\gamma \\mathcal{P}^\\pi v_i\n",
        "\\end{array}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHXtCeIAXons"
      },
      "source": [
        "Script:\n",
        "* Here is the whole iterative policy evaluation algorithm.\n",
        "* The first step computes the rewards and transition probabilities with the previous formulas.\n",
        "* The second step initializes the values.\n",
        "* The third step repeatedly updates the values based on the policy rewards and transitions.\n",
        "* These last two steps are similar to the value iteration algorithm, but there is no max to pick the optimal action since there is only one action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf7R_7cxX7P-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwNF6p5SWsoV"
      },
      "outputs": [],
      "source": [
        "# copied from Implementing Value Iteration video\n",
        "def compute_qT_once(R, P, gamma, v):\n",
        "    return R + gamma * P @ v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoNwBtCeX83Z"
      },
      "outputs": [],
      "source": [
        "# copied from Implementing Value Iteration video\n",
        "def iterate_values_once(R, P, gamma, v):\n",
        "    return np.max(compute_qT_once(R, P, gamma, v), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKQWLNk6awpd"
      },
      "outputs": [],
      "source": [
        "# copied from Implementing Value Iteration video\n",
        "def value_iteration(R, P, gamma, max_iterations=100, tolerance=0.001):\n",
        "    # initial approximation v_0\n",
        "    v_old = np.zeros(R.shape[-1])\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        # compute v_{i+1}\n",
        "        v_new = iterate_values_once(R, P, gamma, v_old)\n",
        "\n",
        "        # check if values did not change much\n",
        "        if np.max(np.abs(v_new - v_old)) < tolerance:\n",
        "            return v_new\n",
        "\n",
        "        v_old = v_new\n",
        "\n",
        "    # return v_{max_iterations}\n",
        "    return v_old"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RajqabzraxsJ"
      },
      "source": [
        "Script:\n",
        "* Here I have a copy of my code for value iteration.\n",
        "* I'm going to copy it now and modify it for iterative policy evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zS-YBw7xYAH0"
      },
      "outputs": [],
      "source": [
        "def iterative_policy_evaluation(R, P, gamma, pi, max_iterations=100, tolerance=0.001):\n",
        "    # factor out action choices using policy.\n",
        "\n",
        "    # probabilistic version for probabilistic policies.\n",
        "    # using einsum to make fancy summations concise.\n",
        "    # R_pi = np.einsum(\"ij,ji->i\", pi, R)\n",
        "    # P_pi = np.einsum(\"ik,kij->ij\", pi, P)\n",
        "\n",
        "    # deterministic version\n",
        "    n = R.shape[-1]\n",
        "    R_pi = R[pi, np.arange(n)]\n",
        "    P_pi = P[pi, np.arange(n),:]\n",
        "\n",
        "    # reshape to one dummy action to reuse previous example code\n",
        "    R_pi = R_pi.reshape(1, *R_pi.shape)\n",
        "    P_pi = P_pi.reshape(1, *P_pi.shape)\n",
        "\n",
        "    # initial approximation v_0\n",
        "    v_old = np.zeros(R.shape[-1])\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        # compute v_{i+1}\n",
        "        v_new = iterate_values_once(R_pi, P_pi, gamma, v_old)\n",
        "\n",
        "        # check if values did not change much\n",
        "        if np.max(np.abs(v_new - v_old)) < tolerance:\n",
        "            return v_new\n",
        "\n",
        "        v_old = v_new\n",
        "\n",
        "    # return v_{max_iterations}\n",
        "    return v_old"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1AB6yWKY5CW"
      },
      "source": [
        "Script:\n",
        "* The most complicated part of iterative policy evaluation is applying the policy $\\pi$ to the rewards and transition probabilities.\n",
        "* I used NumPy's `einsum` function to describe the for loops and summations more concisely, but they are pretty dense.\n",
        "* Loosely speaking, the first string parameter labels both input arrays' dimensions and says which of them should be used as output dimensions.\n",
        "* Any other dimensions that are not output dimensions will be summed up.\n",
        "* After that, a dummy dimension of one is added to the beginning of each shape as if there was exactly one action.\n",
        "* This is not really choice to make, but let's me reuse the previous functions for value iteration.\n",
        "* The rest of the changes are essentially running value iteration on the new rewards and transition probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKTE_KO5Y6ar"
      },
      "source": [
        "## Example Environment\n",
        "\n",
        "<table>\n",
        "<tr><td align=\"right\">0\ud83e\uddca</td><td align=\"right\">1\ud83e\uddca</td><td align=\"right\">2\ud83e\uddca</td><td align=\"right\">3\ud83e\uddca</td><td align=\"right\">4\ud83e\uddca</td><td align=\"right\">5\ud83e\uddca</td><td align=\"right\">6\ud83e\uddca</td></tr>\n",
        "<tr><td align=\"right\">7\ud83e\uddca</td><td align=\"right\">8\ud83e\uddca</td><td align=\"right\">9\ud83e\uddca</td><td align=\"right\">10\ud83e\uddca</td><td align=\"right\">11\ud83e\uddca</td><td align=\"right\">12\ud83e\uddca</td><td align=\"right\">13\ud83e\uddca</td></tr>\n",
        "<tr><td align=\"right\">14\ud83e\uddca</td><td align=\"right\">15\ud83e\uddca</td><td align=\"right\">16\ud83e\uddca</td><td align=\"right\">17\ud83e\uddca</td><td align=\"right\">18\ud83e\uddca</td><td align=\"right\">19\ud83e\uddca</td><td align=\"right\">20\ud83e\uddca</td></tr>\n",
        "<tr><td align=\"right\">21\ud83e\uddca</td><td align=\"right\">22\ud83e\uddca</td><td align=\"right\">23\ud83e\uddca</td><td align=\"right\">24\ud83e\uddca</td><td align=\"right\">25\ud83e\uddca</td><td align=\"right\">26\ud83e\uddca</td><td align=\"right\">27\ud83e\uddca</td></tr>\n",
        "<tr><td align=\"right\">28\ud83e\uddca</td><td align=\"right\">29\ud83e\uddca</td><td align=\"right\">30\ud83e\uddca</td><td align=\"right\">31\ud83e\uddca</td><td align=\"right\">32\ud83e\uddca</td><td align=\"right\">33\ud83e\uddca</td><td align=\"right\">34\ud83e\uddca</td></tr>\n",
        "<tr><td align=\"right\">35\ud83e\uddca</td><td align=\"right\">36\ud83e\uddca</td><td align=\"right\">37\ud83e\uddca</td><td align=\"right\">38\ud83e\uddca</td><td align=\"right\">39\ud83e\uddca</td><td align=\"right\">40\ud83e\uddca</td><td align=\"right\">41\ud83e\uddca</td></tr>\n",
        "<tr><td align=\"right\">42\ud83e\uddca</td><td align=\"right\">43\ud83e\uddca</td><td align=\"right\">44\ud83e\uddca</td><td>45\ud83e\uddca</td><td align=\"right\">46\ud83e\uddca</td><td align=\"right\">47\ud83e\uddca</td><td align=\"right\">48\ud83d\udc1f</td></tr>\n",
        "<tr><td></td></tr>\n",
        "<tr><td align=\"right\">49\u2611\ufe0f</td></tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTaEqC2WZCQ3"
      },
      "source": [
        "Script:\n",
        "* I will reuse the same penguin seeking fish environment as in the value iteration video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1xH7ulWZON9"
      },
      "outputs": [],
      "source": [
        "actions = [\"\u2b06\ufe0f\", \"\u2b07\ufe0f\", \"\u2b05\ufe0f\", \"\u27a1\ufe0f\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxZNoxWtchg0"
      },
      "outputs": [],
      "source": [
        "P = np.zeros(shape=(len(actions), 50, 50))\n",
        "for s in range(48):\n",
        "    # row major order with (0,0) as top left\n",
        "    x = s % 7\n",
        "    y = s // 7\n",
        "\n",
        "    # up action\n",
        "    if y > 0:\n",
        "        P[0, s, s-7] = 1\n",
        "    else:\n",
        "        P[0, s, s] = 1\n",
        "\n",
        "    # down action\n",
        "    if y < 6:\n",
        "        P[1, s, s+7] = 1\n",
        "    else:\n",
        "        P[1, s, s] = 1\n",
        "\n",
        "    # left action\n",
        "    if x > 0:\n",
        "        P[2, s, s-1] = 1\n",
        "    else:\n",
        "        P[2, s, s] = 1\n",
        "\n",
        "    # right action\n",
        "    if x < 6:\n",
        "        P[3, s, s+1] = 1\n",
        "    else:\n",
        "        P[3, s, s] = 1\n",
        "\n",
        "# fish state goes to terminal state\n",
        "P[:,48,49] = 1\n",
        "\n",
        "# stay in terminal state\n",
        "P[:,49,49] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQu6tp8HcoJk"
      },
      "outputs": [],
      "source": [
        "R = np.zeros(shape=(4,50))\n",
        "R[:,48] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ce758CG2eDHn"
      },
      "outputs": [],
      "source": [
        "gamma = 0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9mTliG6ZDcA"
      },
      "source": [
        "## Example Policy\n",
        "\n",
        "<table>\n",
        "<tr><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u2b07\ufe0f</td></tr>\n",
        "<tr><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u2b07\ufe0f</td></tr>\n",
        "<tr><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u2b07\ufe0f</td></tr>\n",
        "<tr><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u2b07\ufe0f</td></tr>\n",
        "<tr><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u2b07\ufe0f</td></tr>\n",
        "<tr><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u2b07\ufe0f</td></tr>\n",
        "<tr><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u27a1\ufe0f</td><td>\u2b07\ufe0f</td></tr>\n",
        "<tr><td></td></tr>\n",
        "<tr><td>\u27a1\ufe0f</td></tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvP7KbXfZFxk"
      },
      "source": [
        "Script:\n",
        "* To demonstrate iterative policy evaluation, I will use this policy.\n",
        "* Go right unless the penguin is on the right edge of the grid.\n",
        "* And go down if on the right edge of the grid.\n",
        "* This is one of many optimal policies for this environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_u4qA-AZRAF",
        "outputId": "f4a76028-92a3-45a4-c413-ba6a96e3bcd8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3,\n",
              "       3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3,\n",
              "       3, 3, 3, 3, 1, 3])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pi = np.zeros(50, dtype=\"int64\")\n",
        "# all states pick action 3 (right)\n",
        "pi[:] = 3\n",
        "# change right edge states to pick action 1 (down)\n",
        "pi[6:49:7] = 1\n",
        "pi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej84JRVeZd_E"
      },
      "source": [
        "Script:\n",
        "* This is a deterministic policy, so it just has one entry specifying the action for each state.\n",
        "* We can visualize quickly to double check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ebwd6SYsfjm",
        "outputId": "486d6d0d-dcc0-4f96-f68d-2b4fcc42215d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([['\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u2b07\ufe0f'],\n",
              "       ['\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u2b07\ufe0f'],\n",
              "       ['\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u2b07\ufe0f'],\n",
              "       ['\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u2b07\ufe0f'],\n",
              "       ['\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u2b07\ufe0f'],\n",
              "       ['\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u2b07\ufe0f'],\n",
              "       ['\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u27a1\ufe0f', '\u2b07\ufe0f']], dtype='<U2')"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.asarray([actions[a] for a in pi[:49]]).reshape(7, 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_gXBi9yseaA"
      },
      "source": [
        "Script:\n",
        "* Now let's run our iterative policy evaluation code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FXo7BX3aC9O",
        "outputId": "d71de4a8-6f91-4cf2-d3b0-2ddc5657df17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.28242954, 0.3138106 , 0.34867844, 0.38742049, 0.43046721,\n",
              "       0.4782969 , 0.531441  , 0.3138106 , 0.34867844, 0.38742049,\n",
              "       0.43046721, 0.4782969 , 0.531441  , 0.59049   , 0.34867844,\n",
              "       0.38742049, 0.43046721, 0.4782969 , 0.531441  , 0.59049   ,\n",
              "       0.6561    , 0.38742049, 0.43046721, 0.4782969 , 0.531441  ,\n",
              "       0.59049   , 0.6561    , 0.729     , 0.43046721, 0.4782969 ,\n",
              "       0.531441  , 0.59049   , 0.6561    , 0.729     , 0.81      ,\n",
              "       0.4782969 , 0.531441  , 0.59049   , 0.6561    , 0.729     ,\n",
              "       0.81      , 0.9       , 0.531441  , 0.59049   , 0.6561    ,\n",
              "       0.729     , 0.81      , 0.9       , 1.        , 0.        ])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "v_pi = iterative_policy_evaluation(R, P, gamma, pi)\n",
        "v_pi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRYnN_k7aBbE"
      },
      "source": [
        "Script:\n",
        "* This looks similar to the optimal values that we saw in the value iteration video.\n",
        "* Let's plot the values to spot check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "qthU6lzoaJrB",
        "outputId": "48faf2e9-3f2d-4885-ce44-2c031a64be0b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(v_pi[:49].reshape(7, 7));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zfwIx_QaHXO"
      },
      "source": [
        "Script:\n",
        "* As we saw before with value iteration, the state values are determined by the number of steps for the penguin to get to the fish."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUaKsNujaNC-"
      },
      "source": [
        "Script: (faculty on screen)\n",
        "* If you have a policy in hand, iterative policy evaluation can compute its values easily.\n",
        "* Compared to value iteration, it will typically be faster since the first thing it does is factor out the actions.\n",
        "* So the iteration part of the algorithm is essentially running with just one action where there were previously $k$.\n",
        "* The catch for this speedup is that iterative policy evaluation needs a policy as input, and only evaluates the given policy.\n",
        "* It does not optimize the policy.\n",
        "* We will cover explicit policy construction in a follow up video."
      ]
    }
  ],
  "metadata": {
    "colab": {},
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}